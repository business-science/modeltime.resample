---
title: "Resampling Panel Time Series Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Resampling Panel Time Series Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    message = FALSE,
    warning = FALSE,
    fig.width = 8, 
    fig.height = 4.5,
    fig.align = 'center',
    out.width='95%', 
    dpi = 100
)
```


Working with __Panel Data__ is a common challenge for business analysts. We often have multiple time series (called Time Series Groups) that have overlapping timestamps. These time series may depend on each other and should be modeled together to take advantage of relationships between correlated time series. 

> <span style="color:blue">__The challenge__ when working with Panel Data is judging how models will perform across time.</span> A single cross-section is not sufficient to instill confidence. 

__Modeltime Resample__ provides a convienent way for generating resample predictions across time for Panel Data, simplifying the model comparison process.

## Panel Data Tutorial Overview

__This is an advanced tutorial.__ Working with Panel Data requires working with multiple time series groups at the same time, and you need to be comfortable setting up the datasets required to generate training sets and forecast sets. I cover working with Panel Data and Time Series Groups in my [High-Performance Time Series Course.](https://university.business-science.io/p/ds4b-203-r-high-performance-time-series-forecasting/) 

## Libraries

Load the following R packages. 

```{r setup}
library(tidymodels)
library(modeltime)
library(modeltime.resample)
library(timetk)
library(tidyverse)
library(tidyquant)
```

## Data

We'll use the `walmart_sales_weekly` dataset from `timetk`. This contains 7 time series groups. 

```{r}
walmart_sales_weekly %>%
  group_by(id) %>%
  plot_time_series(Date, Weekly_Sales, .facet_ncol = 3, .interactive = FALSE)
```

## Data Preparation

We'll create 2 datasets that incorporate a grouping variable:

- __Training Data Set, `data_prepared_tbl`:__ Dataset that contains information on the training region for each time series group

- __Forecast Data Set, `future_tbl`:__ Dataset that contains information on the forecast region for each time series group. We'll extend each time series group by `"3 months"` based on the business forecast needs.

```{r}
# Full = Training + Forecast Datasets
full_data_tbl <- walmart_sales_weekly %>%
  select(id, Date, Weekly_Sales) %>%
  
  # Apply Group-wise Time Series Manipulations
  group_by(id) %>%
  future_frame(
    .date_var   = Date,
    .length_out = "3 months",
    .bind_data  = TRUE
  ) %>%
  ungroup() %>%
  
  # Consolidate IDs
  mutate(id = fct_drop(id))

# Training Data
data_prepared_tbl <- full_data_tbl %>%
  filter(!is.na(Weekly_Sales))

# Forecast Data
future_tbl <- full_data_tbl %>%
  filter(is.na(Weekly_Sales))
```

## Modeling

We'll create:

- __1 Recipe:__ This applies engineered features from calendar variables

- __3 Fitted Models:__ Prophet, XGBoost, and Prophet Boost fitted on the `data_prepared_tbl` dataset

### Recipe 

We'll create a recipe that leverages `step_timeseries_signature()` to generate calendar features. 

```{r}
recipe_spec <- recipe(Weekly_Sales ~ ., data = data_prepared_tbl) %>%
  step_timeseries_signature(Date) %>%
  step_rm(matches("(.iso$)|(.xts$)|(day)|(hour)|(minute)|(second)|(am.pm)")) %>%
  step_mutate(Date_week = factor(Date_week, ordered = TRUE)) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

recipe_spec
```


### Models

Let's generate 3 Models: Prophet, XGBoost, and Prophet Boost. 

#### Prophet

```{r}
wflw_fit_prophet <- workflow() %>%
  add_model(
    prophet_reg() %>% set_engine("prophet") 
  ) %>%
  add_recipe(recipe_spec) %>%
  fit(data_prepared_tbl)
```

#### XGBoost

```{r}
wflw_fit_xgboost <- workflow() %>%
  add_model(
    boost_tree() %>% set_engine("xgboost") 
  ) %>%
  add_recipe(recipe_spec %>% step_rm(Date)) %>%
  fit(data_prepared_tbl)
```

#### Prophet Boost

```{r}
wflw_fit_prophet_boost <- workflow() %>%
  add_model(
    prophet_boost(
      seasonality_daily  = FALSE, 
      seasonality_weekly = FALSE,
      seasonality_yearly = FALSE
    ) %>% 
      set_engine("prophet_xgboost") 
  ) %>%
  add_recipe(recipe_spec) %>%
  fit(data_prepared_tbl)
```

### Organize in a Modeltime Table

Add the 3 fitted models to a Modeltime Table with `modeltime_table()`.

```{r}
model_tbl <- modeltime_table(
  wflw_fit_prophet,
  wflw_fit_xgboost,
  wflw_fit_prophet_boost
)

model_tbl
```

__We can make a Panel Data forecast__, which forecasts all of the time series groups at once. This method is much more efficient than iteratively performing predictions. However, not all time series models respond well to this approach. 

```{r}
# Panel data forecast produces results at once
forecast_panel_tbl <- model_tbl %>%
  modeltime_forecast(
    new_data      = future_tbl,
    # Keep new data allows us keep the ID feature for the 
    #  time series groups
    keep_new_data = TRUE
  ) 
```


__We can visualize the Panel Data forecasts.__ We can see that the Prophet w/ Regressors model is struggling with the Panel Data. However, the Boosted Models are responding well. 

```{r, fig.cap="Forecast Plot for Panel Data Predictions"}
forecast_panel_tbl %>%
  mutate(model_identifier = str_c(.model_id, "_", .model_desc)) %>%
  filter(.key == "prediction") %>%
  ggplot(aes(.index, .value, color = model_identifier)) +
  geom_line(aes(Date, Weekly_Sales), color = "gray30",
            data = data_prepared_tbl) +
  geom_line() + 
  facet_wrap(~ id, scales = "free_y") +
  theme_tq() +
  scale_color_tq() +
  labs(title = "Forecast Plot for Panel Data Predictions",
       caption = "Prophet with Regressors ")
```

# Quantifying Prediction Error

__We've made predictions, but this doesn't tell us how the models will do over time. We need to quantify prediction error.__ To do so, we'll evaluate our models using time series resamples. This technique involves making resamples across time series windows and refitting our models to the resample data sets, producing predictions, and quantifying the error from the predictions. 

### Resampling Panel Data

The first step is to make a resample strategy. Our business objective is to forecast 3 months so we'll use the following strategy:

- assess: 3 months
- skip: 3 months
- cumulative: TRUE. Maximizes the Training Set (alternatively we could do initial = "18 months" to have sliding training sets)
- slice_limit: 6 (keep the 6 largest train/test resamples to prevent too few observations)

This generates 5 resample sets. 

```{r}
walmart_tscv <- data_prepared_tbl %>%
  time_series_cv(
    date_var = Date, 
    assess      = "3 months",
    skip        = "3 months",
    cumulative  = TRUE,
    slice_limit = 6
  )

walmart_tscv
```

We can visualize the resample sets with `plot_time_series_cv_plan()`. They look a little crazy because there are multiple time series groups. The important thing is to make sure the red and blue parts line up as expected in relation to our sampling strategy. 

```{r}
walmart_tscv %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(Date, Weekly_Sales, 
                           .facet_ncol = 2, .interactive = F)
```

### Apply Models to Resamples 

With `model_tbl` (models) and `walmart_tscv` (resamples) in had, we are ready to iteratively fit and predict each of the models on each of the resampling plan sets, producing resample predictions. 

```{r, eval = FALSE}
resample_results <- model_tbl %>%
  modeltime_fit_resamples(
    resamples = walmart_tscv,
    control   = control_resamples(verbose = FALSE)
  )
```

```{r, echo=FALSE}
# write_rds(resample_results, "resample_results.rds")
resample_results <- read_rds("resample_results.rds")
```

A new column (".resample_results") containing the resample predictions has been added to the original `model_tbl`. 

```{r}
resample_results
```

### Evaluate Resample Accuracy

Finally, we can evaluate the accuracy with `modeltime_resample_accuracy()`. We can see that Prophet Boost has the lowest average RMSE compared to the other 2 models. 

__Model Selection:__ If we are interested in a single model, we should select the Prophet Boost model, which has the lowest RMSE of the 3 models. 

```{r}
resample_results %>%
  modeltime_resample_accuracy() %>%
  table_modeltime_accuracy(.interactive = FALSE)
```

We can customize the accuracy results by supplying multiple summary functions in a list. I've supplied `min()` and `max()` to get the range. 

```{r}
resample_results %>%
  modeltime_resample_accuracy(
    summary_fns = list(
      min    = min,
      max    = max
    )
  ) %>%
  table_modeltime_accuracy(.interactive = FALSE)
```

# Wrapup

Working with Panel Data (data with multiple overlapping time series groups) can be challenging due to managing multiple models, overlapping time series groups, and multiple resample sets. __Modeltime Resample makes working with Panel Data much easier.__ We saw how we can evaluate multiple models on varying time series windows. This increased our confidence that the Prophet Boost model was the best model for this data. 



